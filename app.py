"""
Taller IA: Aplicaci贸n Multimodal con OCR y LLMs
Curso: Inteligencia Artificial
Universidad: EAFIT
Profesor: Jorge Padilla
"""

import streamlit as st
import easyocr
from PIL import Image
import numpy as np
from groq import Groq
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Configuraci贸n de la p谩gina
st.set_page_config(
    page_title="Taller IA: OCR + LLM",
    page_icon="",
    layout="wide"
)

st.title("Taller IA: OCR + LLM")
st.markdown("### Aplicaci贸n Multimodal con Visi贸n Artificial y Procesamiento de Lenguaje Natural")
st.markdown("---")

# =============================================================================
# MDULO 1: El Lector de Im谩genes (OCR)
# =============================================================================

st.header(" M贸dulo 1: Extracci贸n de Texto (OCR)")

# Funci贸n para cargar el modelo OCR con cach茅
@st.cache_resource
def load_ocr_reader():
    """
    Carga el modelo EasyOCR en memoria.
    Usa @st.cache_resource para cargar solo una vez y no en cada interacci贸n.
    """
    reader = easyocr.Reader(['es', 'en'])  # Espa帽ol e Ingl茅s
    return reader

# Cargar el lector OCR
with st.spinner("Cargando modelo OCR..."):
    reader = load_ocr_reader()

# Widget para subir imagen
uploaded_file = st.file_uploader(
    "Sube una imagen con texto",
    type=['png', 'jpg', 'jpeg'],
    help="Formatos soportados: PNG, JPG, JPEG"
)

# Procesar imagen si fue subida
if uploaded_file is not None:
    # Mostrar la imagen
    image = Image.open(uploaded_file)
    st.image(image, caption="Imagen subida", use_column_width=True)

    # Convertir imagen a formato numpy array para EasyOCR
    image_np = np.array(image)

    # Bot贸n para extraer texto
    if st.button("Extraer Texto", type="primary"):
        with st.spinner("Extrayendo texto de la imagen..."):
            # Ejecutar OCR
            result = reader.readtext(image_np)

            # Extraer solo el texto de los resultados
            extracted_text = "\n".join([detection[1] for detection in result])

            # Guardar en session_state para persistencia
            st.session_state['extracted_text'] = extracted_text

    # Mostrar texto extra铆do si existe en session_state
    if 'extracted_text' in st.session_state:
        st.success("Texto extra铆do exitosamente")
        st.text_area(
            "Texto extra铆do:",
            value=st.session_state['extracted_text'],
            height=200,
            help="Puedes copiar este texto"
        )

st.markdown("---")

# =============================================================================
# MDULO 2 y 3: Conexi贸n con LLMs (GROQ y Hugging Face)
# =============================================================================

st.header("M贸dulo 2 y 3: An谩lisis con Modelos de Lenguaje")

# Verificar que hay texto extra铆do
if 'extracted_text' in st.session_state and st.session_state['extracted_text']:

    # Crear columnas para la configuraci贸n
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("Configuraci贸n del Modelo")

        # Selector de proveedor
        provider = st.radio(
            "Proveedor de LLM:",
            ["GROQ", "Hugging Face"],
            help="Selecciona el proveedor de API para el an谩lisis"
        )

        if provider == "GROQ":
            # Modelos disponibles en GROQ
            model = st.selectbox(
                "Modelo:",
                [
                    "llama3-8b-8192",
                    "llama3-70b-8192",
                    "mixtral-8x7b-32768",
                    "gemma-7b-it"
                ],
                help="Selecciona el modelo de lenguaje a usar"
            )

        # Tarea a realizar
        task = st.selectbox(
            "Tarea a realizar:",
            [
                "Resumir en 3 puntos clave",
                "Identificar las entidades principales",
                "Traducir al ingl茅s",
                "An谩lisis de sentimiento",
                "Extraer informaci贸n clave"
            ],
            help="Selecciona qu茅 quieres hacer con el texto"
        )

        # Par谩metros ajustables
        st.markdown("**Par谩metros:**")
        temperature = st.slider(
            "Temperature (creatividad):",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="Valores bajos: m谩s determinista. Valores altos: m谩s creativo"
        )

        max_tokens = st.slider(
            "Max Tokens (longitud de respuesta):",
            min_value=50,
            max_value=2000,
            value=500,
            step=50,
            help="Cantidad m谩xima de tokens en la respuesta"
        )

    with col2:
        st.subheader("Resultado del An谩lisis")

        # Bot贸n para analizar
        if st.button("Analizar Texto", type="primary", use_container_width=True):

            # Construir el prompt seg煤n la tarea seleccionada
            task_prompts = {
                "Resumir en 3 puntos clave": "Resume el siguiente texto en 3 puntos clave concisos:",
                "Identificar las entidades principales": "Identifica y lista las entidades principales (personas, lugares, organizaciones, fechas) en el siguiente texto:",
                "Traducir al ingl茅s": "Traduce el siguiente texto al ingl茅s:",
                "An谩lisis de sentimiento": "Analiza el sentimiento del siguiente texto (positivo, negativo, neutral) y explica por qu茅:",
                "Extraer informaci贸n clave": "Extrae la informaci贸n m谩s importante del siguiente texto:"
            }

            system_prompt = task_prompts[task]
            user_text = st.session_state['extracted_text']

            # OPCIN: GROQ
            if provider == "GROQ":
                try:
                    # Verificar API key
                    groq_api_key = os.getenv("GROQ_API_KEY")
                    if not groq_api_key:
                        st.error("No se encontr贸 GROQ_API_KEY en el archivo .env")
                    else:
                        with st.spinner("Analizando con GROQ..."):
                            # Instanciar cliente de GROQ
                            client = Groq(api_key=groq_api_key)

                            # Llamada a la API
                            chat_completion = client.chat.completions.create(
                                messages=[
                                    {
                                        "role": "system",
                                        "content": system_prompt
                                    },
                                    {
                                        "role": "user",
                                        "content": user_text
                                    }
                                ],
                                model=model,
                                temperature=temperature,
                                max_tokens=max_tokens,
                            )

                            # Extraer respuesta
                            response = chat_completion.choices[0].message.content

                            # Mostrar resultado
                            st.markdown("**Respuesta del modelo:**")
                            st.markdown(response)

                            # Informaci贸n adicional
                            st.info(f"Model: {model} | Temperature: {temperature} | Max Tokens: {max_tokens}")

                except Exception as e:
                    st.error(f"Error al conectar con GROQ: {str(e)}")

            # OPCIN: HUGGING FACE

            elif provider == "Hugging Face":
                try:
                    # Verificar API key
                    hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
                    if not hf_api_key:
                        st.error("No se encontr贸 HUGGINGFACE_API_KEY en el archivo .env")
                    else:
                        with st.spinner("Analizando con Hugging Face..."):
                            # Instanciar cliente de Hugging Face
                            client = InferenceClient(token=hf_api_key)

                            # Construir el prompt completo
                            full_prompt = f"{system_prompt}\n\n{user_text}"

                            # Llamada a la API usando chat completion
                            response_text = ""
                            for message in client.chat_completion(
                                model="meta-llama/Meta-Llama-3-8B-Instruct",
                                messages=[
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": user_text}
                                ],
                                max_tokens=max_tokens,
                                temperature=temperature,
                                stream=True,
                            ):
                                response_text += message.choices[0].delta.content

                            # Mostrar resultado
                            st.markdown("**Respuesta del modelo:**")
                            st.markdown(response_text)

                            # Informaci贸n adicional
                            st.info(f"Modelo: Meta-Llama-3-8B | Temperature: {temperature} | Max Tokens: {max_tokens}")

                except Exception as e:
                    st.error(f"Error al conectar con Hugging Face: {str(e)}")
                    st.info("Tip: Aseg煤rate de que tu token de Hugging Face tenga permisos de 'Read' para Inference.")

else:
    st.info(" Primero extrae texto de una imagen en la secci贸n superior")

# FOOTER

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p><strong>Taller IA: Aplicaci贸n Multimodal con OCR y LLMs</strong></p>
    <p>Inteligencia Artificial | Universidad EAFIT | Prof. Jorge Padilla</p>
</div>
""", unsafe_allow_html=True)


# SIDEBAR: Informaci贸n y ayuda


with st.sidebar:
    st.header("癸 Informaci贸n")

    st.markdown("""
    ### C贸mo usar esta aplicaci贸n:

    1. **Sube una imagen** con texto
    2. **Extrae el texto** con OCR
    3. **Selecciona un proveedor** (GROQ o Hugging Face)
    4. **Elige una tarea** a realizar
    5. **Ajusta los par谩metros** si lo deseas
    6. **Analiza** el texto extra铆do

    ### Sobre los par谩metros:

    **Temperature:** Controla la creatividad
    - Bajo (0.1-0.5): Respuestas m谩s deterministas
    - Medio (0.6-1.0): Balance
    - Alto (1.1-2.0): M谩s creativo/aleatorio

    **Max Tokens:** Longitud m谩xima de la respuesta

    ### APIs requeridas:
    - GROQ API Key
    - Hugging Face Token
    """)

    st.markdown("---")

    # Verificar estado de las API keys
    st.markdown("### Estado de API Keys:")

    groq_key = os.getenv("GROQ_API_KEY")
    hf_key = os.getenv("HUGGINGFACE_API_KEY")

    if groq_key:
        st.success(" GROQ configurado")
    else:
        st.error("GROQ no configurado")

    if hf_key:
        st.success(" Hugging Face configurado")
    else:
        st.error(" Hugging Face no configurado")

    if not groq_key or not hf_key:
        st.warning("Configura tus claves en el archivo .env")